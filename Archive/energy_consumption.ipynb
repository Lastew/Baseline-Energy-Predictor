{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pd.set_option('display.float_format', lambda x: '%.5f'%x)\n",
    "#from datetime import datetime as dt\n",
    "\n",
    "# Visualization packages:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import all necessary libraries.\n",
    "\n",
    "import warnings\n",
    "# import plots as p\n",
    "import function as f\n",
    "# import data_testing as dt\n",
    "# import data_prep as dp\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import statsmodels.api as sm\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy savings has two keys elements:\n",
    "- forecasting energy consumption without any energy saving measures \n",
    "- forecasting energy consumption after certain energy saving measures\n",
    "\n",
    "This notebook predict "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "train = pd.read_csv(\"data/train.csv\", parse_dates=['timestamp'])\n",
    "weather_train = pd.read_csv(\"data/weather_train.csv\", parse_dates=['timestamp'])\n",
    "metadata = pd.read_csv(\"data/building_metadata.csv\")\n",
    "\n",
    "print('Size of training data', train.shape)\n",
    "print('Mem. size of original training data', train.memory_usage().sum()/1024**2)\n",
    "print('----------------------------------')\n",
    "print('Size of training weather data', weather_train.shape)\n",
    "print('Mem. size of original training weather data', weather_train.memory_usage().sum()/1024**2)\n",
    "print('----------------------------------')\n",
    "print('Size of building meta data', metadata.shape)\n",
    "print('Mem. size of original building meta data', metadata.memory_usage().sum()/1024**2)\n",
    "\n",
    "# df.memory_usage().sum() / 1024**2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reducing memory\n",
    "print('Mem. size of reduced training data', train_df.shape)\n",
    "train_df = f.reduce_mem_usage(train)\n",
    "print('----------------------------------')\n",
    "print('Mem. size of reduced training weather data', weather_train_df.shape)\n",
    "weather_train_df = f.reduce_mem_usage(weather_train)\n",
    "print('----------------------------------')\n",
    "print('Mem. size of reduced building meta data', building_meta_df.shape)\n",
    "building_meta_df = f.reduce_mem_usage(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***there are 15 different site_ids***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(building_meta_df.shape)\n",
    "building_meta_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_train_df.shape)\n",
    "weather_train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()/len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.isna().sum()#/len(building_meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.isna().sum()#/len(weather_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacing meter's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['meter'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meter'].replace({0:\"Electricity\",1:\"ChilledWater\", 2:\"Steam\",3:\"HotWater\"}, inplace=True)\n",
    "train_df['meter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "sns.set(style='darkgrid')\n",
    "# sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "# sns.set_context('talk')\n",
    "sns.countplot(train_df['meter'], order=train_df['meter'].value_counts().index)\n",
    "plt.title('Distribution of Meter Id Code')\n",
    "plt.xlabel('Meter Id Code')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"There are {} unique Buildings in the training data\".format(train_df['building_id'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like it is mentioned in the competition description, each building may or may not have all 4 meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('meter')['meter_reading'].agg(['min','max','mean','median', 'count', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata\n",
    "sns.countplot(building_meta_df['primary_use'], order=building_meta_df['primary_use'].value_counts().index)\n",
    "plt.title('Distribution of Primary use of Buildings')\n",
    "plt.xlabel('Building types')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building floor count\n",
    "sns.countplot(building_meta_df['floor_count'], order=building_meta_df['floor_count'].value_counts().index)\n",
    "plt.title('Floor counts')\n",
    "plt.xlabel('Floors')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year built \n",
    "sns.countplot(building_meta_df['year_built'], order=building_meta_df['year_built'].value_counts().index)\n",
    "plt.title('Year built')\n",
    "plt.xlabel('Years')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter(metadata, y=\"square_feet\", x=\"year_built\", color=\"primary_use\", size=\"square_feet\")\n",
    "# fig.update_layout(showlegend=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df]:\n",
    "    df['month'] = df['timestamp'].dt.month.astype(\"uint8\")\n",
    "    df['day_of_month'] = df['timestamp'].dt.day.astype(\"uint8\")\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n",
    "    df['hour'] = df['timestamp'].dt.hour.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_timestamp(df):\n",
    "    df['month'] = df['timestamp'].dt.month.astype('uint8')\n",
    "    df['day_of_month'] = df['timestamp'].dt.day.astype('uint8')\n",
    "    df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('uint8')\n",
    "    df['hour'] = df['timestamp'].dt.hour.astype('uint8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by meter and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = split_timestamp(train_df)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "train_df.groupby(['meter', 'month'])['meter_reading'].agg(['max', 'mean', 'median', 'count', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Month', 'DayOfMonth', 'DayOfWeek', 'Hour'], inplace=True)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby(['meter', 'day_of_week'])['meter_reading'].agg(['max', 'mean', 'median', 'count', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(np.log1p(train_df['meter_reading']), kde=False)\n",
    "plt.title('Distribution of log of meter reading variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train_df[train_df['meter'] == \"Electricity\"]['meter_reading']);\n",
    "plt.title(\"Boxplot of Meter Reading Variable for the Meter Type: Electricity\");\n",
    "# We can see a few outliers here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train_df[train_df['meter'] == \"ChilledWater\"]['meter_reading']);\n",
    "plt.title(\"Boxplot of Meter Reading Variable for the Meter Type: ChilledWater\");\n",
    "# Not many outliers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train_df[train_df['meter'] == \"HotWater\"]['meter_reading']);\n",
    "plt.title(\"Boxplot of Meter Reading Variable for the Meter Type: HotWater\");\n",
    "# We can see a single value that is way off from the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(train_df[train_df['meter'] == \"Steam\"]['meter_reading']);\n",
    "plt.title(\"Boxplot of Meter Reading Variable for the Meter Type: Steam\");\n",
    "# We can see a few outliers here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the dependent variable to logarithmic scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['meter_reading'] = np.log1p(train_df['meter_reading'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df[train_df['meter'] == \"Electricity\"]['meter_reading'],kde=False)\n",
    "plt.title(\"Distribution of Meter Reading per MeterID code: Electricity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df[train_df['meter'] == \"ChilledWater\"]['meter_reading'],kde=False)\n",
    "plt.title(\"Distribution of Meter Reading per MeterID code: Chilledwater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df[train_df['meter'] == \"Steam\"]['meter_reading'],kde=False)\n",
    "plt.title(\"Distribution of Meter Reading per MeterID code: Steam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_df[train_df['meter'] == \"HotWater\"]['meter_reading'],kde=False)\n",
    "plt.title(\"Distribution of Meter Reading per MeterID code: Hotwater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.info()\n",
    "# Missing values in year_built and floor_count variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['site_id','primary_use','year_built','floor_count','building_id']\n",
    "for col in cols:\n",
    "    print (\"Number of Unique Values in the {} column are:\".format(col),building_meta_df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['site_id','primary_use','floor_count']\n",
    "for col in cols:\n",
    "    print (\"Unique Values in the {} column are:\".format(col),building_meta_df[col].unique())\n",
    "    print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(building_meta_df['site_id'])\n",
    "plt.title(\"Count of Site_id in the Metadata table\")\n",
    "plt.xlabel(\"Site_Id\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "building_meta_df['primary_use'].value_counts().sort_values().plot(kind='bar')\n",
    "plt.title(\"Count of Primary_Use Variable in the Metadata table\")\n",
    "plt.xlabel(\"Primary Use\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90);\n",
    "# Education, Office, Entertainment/Public Assembly, Public Services, Lodging/Residential form the bulk of Primary Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df['square_feet'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(building_meta_df['square_feet']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df['square_feet'] = np.log1p(building_meta_df['square_feet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(building_meta_df['square_feet'])\n",
    "plt.title(\"Distribution of Square Feet variable of Metadata Table\")\n",
    "plt.xlabel(\"Area in Square Feet\")\n",
    "plt.ylabel(\"Frequency\");\n",
    "# Looks like a normal distribution distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(building_meta_df['square_feet'])\n",
    "plt.title(\"Box Plot of Square Feet Variable\");\n",
    "# There are a few outliers visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.groupby('primary_use')['square_feet'].agg(['mean','median','count']).sort_values(by='count')\n",
    "# Parking has the highest average are although the count is less.\n",
    "# Education has the highest count as can be seen in the countplot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df['year_built'].value_counts().sort_values().plot(kind='bar',figsize=(15,6))\n",
    "plt.xlabel(\"Year Built\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Year Built Variable\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df['floor_count'].value_counts(dropna=False).sort_index().plot(kind='bar',figsize=(8,6))\n",
    "plt.xlabel(\"Number of Floors\")\n",
    "plt.ylabel(\"Count of Buildings\");\n",
    "# Lot of missing values here as well\n",
    "# Maximum number of floors is 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.groupby('floor_count')['square_feet'].agg(['count','mean','median']).sort_values(by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta_df.groupby('primary_use')['square_feet'].agg(['count','mean','median']).sort_values(by='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['year_built'].fillna(-999, inplace=True)\n",
    "metadata['year_built'] = metadata['year_built'].astype('int16')\n",
    "metadata['floor_count'].fillna(-999, inplace=True)\n",
    "metadata['floor_count'] = metadata['floor_count'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data (Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\n",
    "for col in cols:\n",
    "    print (\" Minimum Value of {} column is {}\".format(col,weather_train[col].min()))\n",
    "    print (\" Maximum Value of {} column is {}\".format(col,weather_train[col].max()))\n",
    "    print (\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df.isna().sum()/len(weather_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train_df[['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather_train['timestamp'].min())\n",
    "print(weather_train['timestamp'].max())\n",
    "print(len( weather_train['timestamp']))\n",
    "# This data is from 1st Jan to 31st Dec 2016, similar to the timestamp of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed']\n",
    "for ind,col in enumerate(weather_train[cols]):\n",
    "    plt.figure(ind)\n",
    "    sns.distplot(weather_train[col].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed']\n",
    "for ind,col in enumerate(weather_train[cols]):\n",
    "    plt.figure(ind)\n",
    "    sns.boxplot(weather_train[col].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time \n",
    "# %%time\n",
    "train_data = pd.merge(train_df,building_meta_df,on='building_id',how='left')\n",
    "# test  = pd.merge(test,metadata,on='building_id',how='left')\n",
    "print (\"Training Data Shape {}\".format(train_data.shape))\n",
    "# print (\"Testing Data Shape {}\".format(test.shape))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "train = pd.merge(train_data,weather_train,on=['site_id','timestamp'],how='left')\n",
    "# test  = pd.merge(test,weather_test,on=['site_id','timestamp'],how='left')\n",
    "print (\"Training Data Shape {}\".format(train.shape))\n",
    "# print (\"Testing Data Shape {}\".format(test.shape))\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\n",
    "for col in cols:\n",
    "    train[col].fillna(train[col].mean(),inplace=True)\n",
    "#     test[col].fillna(test[col].mean(),inplace=True)\n",
    "    \n",
    "del metadata, weather_train#, weather_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for df in [train]:#, test]:\n",
    "    df['weekend'] = np.where((df['day_of_week'] == 5) | (df['day_of_week'] == 6), 1, 0)\n",
    "    df['air_temperature'] = df['air_temperature'].astype('float16')\n",
    "    df['cloud_coverage'] = df['cloud_coverage'].astype(\"float16\")\n",
    "    df['dew_temperature'] = df['dew_temperature'].astype('float16')\n",
    "    df['precip_depth_1_hr'] = df['precip_depth_1_hr'].astype('float32')\n",
    "    df['sea_level_pressure'] = df['sea_level_pressure'].astype('float32')\n",
    "    df['wind_direction'] = df['wind_direction'].astype('float32')\n",
    "    df['wind_speed'] = df['wind_speed'].astype('float16')\n",
    "    df['square_feet'] = df['square_feet'].astype(\"float32\")\n",
    "    df['building_id'] = df['building_id'].astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the inputs (X) are reshaped into the 3D format expected by LSTMs, namely [samples, timesteps, features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "# this is a layeer with 50 neurons and input_shape is 1 time searies haveing 8 features \n",
    "model.add(Dense(1)) # Dense is how many output we would like to have \n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -7:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -7:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[30],\n",
       "       [40],\n",
       "       [50],\n",
       "       [60],\n",
       "       [70]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = {'x':[1,2,3,4,5], 'y':[30,40,50,60,70], 'z':[70,80,90,100,110]}\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(val)\n",
    "dataset = df[['y']].values\n",
    "print(type(dataset))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.reshape(dataset, (-1,1))\n",
    "type(dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  ],\n",
       "       [0.25],\n",
       "       [0.5 ],\n",
       "       [0.75],\n",
       "       [1.  ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "X_train_size = len(dataset) * 0.8\n",
    "X_test_size = len(dataset) - X_train\n",
    "print(X_train_size, X_test_size)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-ff32ea8c8fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_train_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "X_train, X_test = dataset[0:X_train_size,:], dataset[X_train_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.57142857],\n",
       "       [0.14285714],\n",
       "       [0.71428571],\n",
       "       [0.28571429],\n",
       "       [0.85714286],\n",
       "       [0.42857143],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = scaler.fit_transform(dataset)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
