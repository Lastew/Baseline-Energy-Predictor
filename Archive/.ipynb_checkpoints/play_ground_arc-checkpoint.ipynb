{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:11.488841Z",
     "start_time": "2019-11-23T03:16:07.707079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.3f'%x)\n",
    "# pd.set_option('float_format', '{:f}'.format)\n",
    "#from datetime import datetime as dt\n",
    "\n",
    "# Visualization packages:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import warnings\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from IPython.display import HTML # to see everything\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import sklearn and statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:19.033172Z",
     "start_time": "2019-11-23T03:16:17.736328Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sklearn \n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import Statsmodels \n",
    "# from statsmodels.tsa.api import VAR\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tools.eval_measures import rmse, aic\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.graphics.gofplots import qqplot\n",
    "# import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -user statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:25.002564Z",
     "start_time": "2019-11-23T03:16:21.728680Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "\n",
    "# import Sequential\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                               TensorBoard, ReduceLROnPlateau, CSVLogger)\n",
    "# print \n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plots as p\n",
    "import supervised_learning as sl\n",
    "# helper_functions as hf \n",
    "# import data_testing as dt\n",
    "# import data_prep as dp\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(x_train))\n",
    "# print(x_train.shape)\n",
    "# # print(type(x_valid))\n",
    "# # print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data -- Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:41.341693Z",
     "start_time": "2019-11-23T03:16:41.093606Z"
    }
   },
   "outputs": [],
   "source": [
    "office = pd.read_csv('data_folder/office_data.csv')#, index_col=['timestamp'], parse_dates=['timestamp'])\n",
    "office = pd.read_csv('data_folder/office_1249.csv', index_col=['timestamp'], parse_dates=['timestamp'])\n",
    "usecols =['meter', 'meter_reading', 'air_temperature', 'cloud_coverage', 'dew_temperature','precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "office = office[usecols]\n",
    "office.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# office[office.building_id ==1249].shape\n",
    "office.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:41.341693Z",
     "start_time": "2019-11-23T03:16:41.093606Z"
    }
   },
   "outputs": [],
   "source": [
    "office_temp = office.ffill()\n",
    "office_data = office_temp.bfill()\n",
    "office_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T03:16:45.647126Z",
     "start_time": "2019-11-23T03:16:45.549780Z"
    }
   },
   "outputs": [],
   "source": [
    "# electricity\n",
    "elec_consumption = office_data[office_data['meter']==0]\n",
    "elec_consumption.drop(columns=['meter'], inplace=True)\n",
    "print(elec_consumption.shape)\n",
    "elec_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataframe has a zero element\n",
    "# elec_consumption['wind_direction'].isin([0]).any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box plots and outlier detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large = 22\n",
    "med = 16\n",
    "small = 12\n",
    "params = {'axes.titlesize': large,\n",
    "          'legend.fontsize': large, # med\n",
    "          'figure.figsize': (16, 10),\n",
    "          'axes.labelsize': med,\n",
    "          'xtick.labelsize': large, #med,\n",
    "          'ytick.labelsize': large, #med,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "# tips = sns.load_dataset(\"tips\")\n",
    "elec_consumption['meter_reading'].replace(to_replace=0, method='ffill', inplace=True)\n",
    "ax = sns.boxplot(x=elec_consumption[\"meter_reading\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_consumption['meter_reading'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:03:29.737967Z",
     "start_time": "2019-11-23T04:03:29.725217Z"
    }
   },
   "outputs": [],
   "source": [
    "target_names = ['meter_reading', 'air_temperature']\n",
    "# target_names = list(df_elc.columns) #['meter_reading', 'air_temperature'] # list(df_elc.columns)\n",
    "shift_days = 1\n",
    "shift_steps = shift_days * 24 # number of hours.\n",
    "df_targets = elec_consumption[target_names].shift(-shift_steps)\n",
    "df_targets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:03:34.625974Z",
     "start_time": "2019-11-23T04:03:34.610857Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the raws from 24- 27\n",
    "elec_consumption[target_names].iloc[24:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:03:56.035847Z",
     "start_time": "2019-11-23T04:03:56.026258Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_data\n",
    "x_data = elec_consumption.values[0:-shift_steps]\n",
    "print(type(x_data))\n",
    "print('    Feature Shape:', x_data.shape)\n",
    " \n",
    "# y_data\n",
    "y_data = df_targets.values[:-shift_steps]\n",
    "print(type(y_data))\n",
    "print('    Target Shape:', y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:03:59.545086Z",
     "start_time": "2019-11-23T04:03:59.538163Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the input-signals for the training- and test-sets:\n",
    "num_data = len(x_data)\n",
    "train_split = 0.9\n",
    "num_train = int(train_split * num_data)\n",
    "num_test = num_data - num_train\n",
    "print('Training observations:', num_train)\n",
    "print('Validation observations:', num_test)\n",
    "x_train = x_data[0:num_train]\n",
    "x_test  = x_data[num_train:]\n",
    "print('Total observations:', (len(x_train) + len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:04:02.292548Z",
     "start_time": "2019-11-23T04:04:02.283410Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:06:47.022531Z",
     "start_time": "2019-11-23T04:06:47.014304Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the output-signals for the training- and test-sets:\n",
    "y_train = y_data[0:num_train]\n",
    "y_test = y_data[num_train:]\n",
    "print('Total test observations:', (len(y_train) + len(y_test)))\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:07:56.671856Z",
     "start_time": "2019-11-23T04:07:56.664855Z"
    }
   },
   "outputs": [],
   "source": [
    "num_x_signals = x_data.shape[1] # 8\n",
    "num_y_signals = y_data.shape[1] # 2\n",
    "print('Number of input-signals:', num_x_signals, 'and shape:', x_train.shape )\n",
    "print('Number of output-signals:', num_y_signals, 'and shape', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:08:07.276799Z",
     "start_time": "2019-11-23T04:08:07.268386Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data[24:27] # I shifted the data by one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:11:53.408877Z",
     "start_time": "2019-11-23T04:11:53.398046Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Min and Max x_train data')\n",
    "print('    Min:', np.min(x_train))\n",
    "print('    Max:', np.max(x_train))\n",
    "x_scaler = MinMaxScaler()\n",
    "x_scaled_train = x_scaler.fit_transform(x_train)\n",
    "print('Min and Max x_train_scaled data')\n",
    "print('    Min:', np.min(x_scaled_train))\n",
    "print('    Max:', np.max(x_scaled_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:12:57.741506Z",
     "start_time": "2019-11-23T04:12:57.729025Z"
    }
   },
   "outputs": [],
   "source": [
    "x_scaled_test = x_scaler.transform(x_test)\n",
    "print('Min and Max x_test data')\n",
    "print('    Min:', np.min(x_test))\n",
    "print('    Max:', np.max(x_test))\n",
    "\n",
    "# target MinMaxScaler\n",
    "y_scaler = MinMaxScaler()\n",
    "y_scaled_train = y_scaler.fit_transform(y_train)\n",
    "y_scaled_test = y_scaler.transform(y_test)\n",
    "print('Min and Max y_test_scaled data')\n",
    "print('    Min:', np.min(y_scaled_test))\n",
    "print('    Max:', np.max(y_scaled_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T04:24:01.285134Z",
     "start_time": "2019-11-23T04:24:01.273430Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x_scaled_train.shape)\n",
    "print(y_scaled_train.shape)\n",
    "print('-------')\n",
    "print(x_scaled_test.shape)\n",
    "print(y_scaled_test.shape)\n",
    "# y_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have now is one long time series with 20 input signals so that each time step has 20 input signals and 3 output signals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:53:04.230747Z",
     "start_time": "2019-11-23T13:53:04.221242Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x_scaled_train.shape)\n",
    "print(y_scaled_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:53:06.282885Z",
     "start_time": "2019-11-23T13:53:06.267907Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_scaled_train[idx:idx+sequence_length]\n",
    "            y_batch[i] = y_scaled_train[idx:idx+sequence_length]\n",
    "        \n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above funcion will break the long time series signal into small sub sequences and create a batch so that we can use that for training the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "reshape input to be [samples, time steps, features]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:16.228241Z",
     "start_time": "2019-11-23T13:54:16.218920Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "batch_size = 30 #72\n",
    "sequence_length = shift_steps # 24 * 7\n",
    "\n",
    "# create the batch-generator\n",
    "generator = batch_generator(batch_size, sequence_length)\n",
    "\n",
    "# test the batch-generator to see if it works \n",
    "x_batch, y_batch = next(generator)\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get a batch output where we have a batch size of xxx sequences, each sequence has yyy, and zzz input signals and zzz' output signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:20.428778Z",
     "start_time": "2019-11-23T13:54:20.232415Z"
    }
   },
   "outputs": [],
   "source": [
    "signal_list = list(elec_consumption.columns)\n",
    "print('length of signals:', len(signal_list))\n",
    "\n",
    "# signal plot for electricity consumption\n",
    "batch = 0   # First sequence in the batch.\n",
    "signal = 0  # First signal from the 8 input-signals.\n",
    "seq = x_batch[batch, :, signal]\n",
    "plt.plot(seq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:22.350413Z",
     "start_time": "2019-11-23T13:54:22.151338Z"
    }
   },
   "outputs": [],
   "source": [
    "# y plot for electricity consumption\n",
    "seq = y_batch[batch, :, signal] # only two output signals\n",
    "plt.plot(seq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:31.213719Z",
     "start_time": "2019-11-23T13:54:31.209482Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_data = (np.expand_dims(x_scaled_test, axis=0),\n",
    "                   np.expand_dims(y_scaled_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_test_validation', validation_data[0].shape)\n",
    "print('y_test_validation', validation_data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Recurrent Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:35.586742Z",
     "start_time": "2019-11-23T13:54:35.582063Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initalize the RNN\n",
    "Now let us inistantiate an RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear a model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing RNN \n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the LSTM layers and some Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "model.add(LSTM(units=50, return_sequences=True,\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "if False:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "    # Maybe use lower init-ranges.\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05) # I ca\n",
    "\n",
    "    model.add(Dense(num_y_signals,\n",
    "                    activation='linear',\n",
    "                    kernel_initializer=init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the loss_mse_warmup function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 41-63 supervised_learning.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "# optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=loss_mse_warmup, optimizer=optimizer, metrics=['acc'])\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_checkpoint = '23_checkpoint.keras'\n",
    "path_checkpoint = 'weights/model_weights.best.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                     monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False) #'./23_logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('logs/training_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback lists \n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr,\n",
    "             csv_logger]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in h5 format\n",
    "# model.save('weights/first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "lastew = load_model('weights/first_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set\n",
    "x_scaled_train.shape, y_scaled_train.shape\n",
    "x_sis = np.expand_dims(x_scaled_train, axis=0)\n",
    "y_sis = np.expand_dims(y_scaled_train, axis=0)\n",
    "lastew.evaluate(x_sis, y_sis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test set\n",
    "lastew.evaluate(validation_data[0], validation_data[1])\n",
    "print('validation set should be a 3D arrray', \n",
    "      validation_data[0].shape, validation_data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_scaled_train.shape)\n",
    "print(y_scaled_train.shape)\n",
    "print('-----')\n",
    "print(x_scaled_test.shape)\n",
    "print(y_scaled_test.shape)\n",
    "type(x_scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x=np.expand_dims(x_scaled_test, axis=0),\n",
    "                        y=np.expand_dims(y_scaled_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have several metrics you can use this instead.\n",
    "if False:\n",
    "    for res, metric in zip(result, model.metrics_names):\n",
    "        print(\"{0}: {1:.3e}\".format(metric, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load plot comparision plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 30-66 visualization.py\n",
    "def plot_comparison(start_idx, length=100, train=True):\n",
    "    \n",
    "    if train:\n",
    "        # Use training-data.\n",
    "        x = x_scaled_train\n",
    "        y_true = y_train\n",
    "    else:\n",
    "        # Use test-data.\n",
    "        x = x_scaled_train\n",
    "        y_true = y_test\n",
    "    \n",
    "    end_idx = start_idx + length\n",
    "    \n",
    "    x = x[start_idx:end_idx]\n",
    "    y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
    "    \n",
    "    for signal in range(len(target_names)):\n",
    "        signal_pred = y_pred_rescaled[:, signal]\n",
    "\n",
    "        signal_true = y_true[:, signal]\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        plt.plot(signal_true, label='true')\n",
    "        plt.plot(signal_pred, label='pred')\n",
    "       \n",
    "        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "       \n",
    "        plt.ylabel(target_names[signal])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=4000, length=1000, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=0, length=1000, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 2nd_LSTM Model\n",
    "this model uses an upgrade on batch size\n",
    "- softmax activation\n",
    "- optimizer adam \n",
    "- batch size 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:54:16.228241Z",
     "start_time": "2019-11-23T13:54:16.218920Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "batch_size_2 = 30\n",
    "# sequence_length = shift_steps # 24 * 7\n",
    "\n",
    "# create the batch-generator\n",
    "generator_2 = batch_generator(batch_size_2, sequence_length)\n",
    "\n",
    "# test the batch-generator to see if it works \n",
    "x_batch_2, y_batch_2 = next(generator_2)\n",
    "print(x_batch_2.shape)\n",
    "print(y_batch_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "# batch_size_2 = 30\n",
    "# sequence_length = shift_steps # 24 * 7\n",
    "\n",
    "# create the batch-generator\n",
    "# generator_2 = batch_generator(batch_size_2, sequence_length)\n",
    "\n",
    "# test the batch-generator to see if it works \n",
    "x_batch, y_batch = next(generator)\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear a model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing RNN \n",
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the LSTM layers and some Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "model2.add(LSTM(units=50, return_sequences=True,\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the second LSTM layer and some Dropout regularisation\n",
    "model2.add(LSTM(units=50, return_sequences=True))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "if False:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "    # Maybe use lower init-ranges.\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05) # I ca\n",
    "\n",
    "    model2.add(Dense(num_y_signals,\n",
    "                    activation='linear',\n",
    "                    kernel_initializer=init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)\n",
    "# optimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=loss_mse_warmup, optimizer=optimizer, metrics=['acc'])\n",
    "model2.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the RNN to the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fbprophet --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fbprophet import Prophet\n",
    "# import logging \n",
    "# logging.getLogger().setLevel(Logging.ERRORProPr)\n",
    "# prophet = Prophet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install progressbar2 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Fedaral Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_elc.copy()\n",
    "df = df.reset_index()\n",
    "df['days'] = df.timestamp.apply(lambda x:1 if x.dayofweek > 5 else 0)\n",
    "df.days.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = calendar()\n",
    "holidays = cal.holidays(start = df.timestamp.min(), end = df.timestamp.max())\n",
    "df[\"holiday\"] = df.timestamp.isin(holidays).astype('int')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X'Mas Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmas = (df.timestamp >= pd.to_datetime(\"12/20/2016\")) & (df.timestamp <= pd.to_datetime('12/27/2016'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model Architecture\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(units=100,\n",
    "#               return_sequences=True,\n",
    "#               input_shape=(None, num_x_signals,)))\n",
    "\n",
    "# model.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, I instantiate the Sequential class. This will be my model class and I will add LSTM, Dropout and Dense layers to this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:55:44.457463Z",
     "start_time": "2019-11-23T13:55:44.210488Z"
    }
   },
   "outputs": [],
   "source": [
    "# model Architecture\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50,\n",
    "              return_sequences=True,\n",
    "#               input_shape=(None, num_x_signals,)\n",
    "                 input_shape=(None, num_x_signals,)\n",
    "                ))\n",
    "\n",
    "model_lstm.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "# model_s.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of the last layer is a tensor with an aritrary batch size and arbitrary sequance length and two output length signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the other code\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=100,\n",
    "               return_sequences=True,\n",
    "               input_shape=(x_batch.shape[1], x_batch.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(num_y_signals))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(x_batch, y_batch, epochs=2, \n",
    "                    batch_size=70, validation_data=(X_test, Y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "# # Training Phase\n",
    "\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=2,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=20, batch_size=70, validation_data=(X_test, Y_test), \n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "# # Training Phase\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generator(features, labels, batch_size):\n",
    "    \n",
    "#     # Create empty arrays to contain batch of features and labels#\n",
    "#     batch_features = np.zeros((batch_size, 64, 64, 3))\n",
    "#     batch_labels = np.zeros((batch_size,1))\n",
    "#     while True:\n",
    "#         for i in range(batch_size):\n",
    "#             # choose random index in features\n",
    "#             index= random.choice(len(features),1)\n",
    "#             batch_features[i] = some_processing(features[index])\n",
    "#             batch_labels[i] = labels[index]\n",
    "#         yield batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using series_to_supervised function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervised_learning as ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_elc.values\n",
    "print(values.shape)\n",
    "print(type(df_elc.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring all the data is a float \n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "print('Min and Max values data')\n",
    "print('    Min:', np.min(values))\n",
    "print('    Max:', np.max(values))\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "print('Min and Max scaled data')\n",
    "print('    Min:', np.min(scaled))\n",
    "print('    Max:', np.max(scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame as supervised learning\n",
    "reframed = ls.series_to_supervised(scaled, 0, 2)\n",
    "reframed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_elc.iloc[:,0:3].head(20)\n",
    "dataset = df.values\n",
    "X_train, y_train = split_sequences(dataset, 3, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=4, #validation_data=(X_test, Y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "# Training Phase\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the data\n",
    "for i in range(len(X_train)):\n",
    "    print(X_train[i], y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the other code\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=100,\n",
    "               return_sequences=True,\n",
    "               input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(X_train.shape[2]))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=2, \n",
    "                    batch_size=4, validation_data=(X_test, Y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network\n",
    "n_batch = len(X_train)\n",
    "n_epoch = 100\n",
    "n_neurons = 10\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X_train.shape[1], \n",
    "                                             X_train.shape[2]), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
