{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:33:53.640022Z",
     "start_time": "2019-11-25T17:33:50.366871Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data analysis packages:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.float_format', lambda x: '%.3f'%x)\n",
    "#from datetime import datetime as dt\n",
    "\n",
    "# Visualization packages:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.dates import DateFormatter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import warnings\n",
    "import itertools\n",
    "import datetime as dt\n",
    "from IPython.display import HTML # to see everything\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import sklearn and statsmodels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:33:57.831268Z",
     "start_time": "2019-11-25T17:33:56.269490Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sklearn \n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# import Statsmodels \n",
    "# from statsmodels.tsa.api import VAR\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tools.eval_measures import rmse, aic\n",
    "# from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "# from statsmodels.graphics.gofplots import qqplot\n",
    "# import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:02.901244Z",
     "start_time": "2019-11-25T17:33:59.352704Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "\n",
    "# import Sequential\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "# print \n",
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import my functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:04.766629Z",
     "start_time": "2019-11-25T17:34:04.652722Z"
    }
   },
   "outputs": [],
   "source": [
    "# import plots as p\n",
    "# import function as f\n",
    "# import data_testing as dt\n",
    "# import data_prep as dp\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    \"\"\"Repeats the same pattern at each period\"\"\"\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "time = np.arange(4 * 365 + 1, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)  \n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5\n",
    "\n",
    "# Create the series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)\n",
    "\n",
    "split_time = 1000\n",
    "time_train = time[:split_time]\n",
    "x_train = series[:split_time]\n",
    "time_valid = time[split_time:]\n",
    "x_valid = series[split_time:]\n",
    "\n",
    "window_size = 20\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(x_train.shape)\n",
    "print(type(x_valid))\n",
    "print(x_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data -- Electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:23.169684Z",
     "start_time": "2019-11-25T17:34:22.864098Z"
    }
   },
   "outputs": [],
   "source": [
    "office = pd.read_csv('data_folder/office_1249.csv', index_col=['timestamp'], parse_dates=['timestamp'])\n",
    "usecols =['meter', 'meter_reading', 'air_temperature', 'cloud_coverage', 'dew_temperature','precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n",
    "# office = office[office[usecols]]\n",
    "office = office[usecols]\n",
    "office_temp = office.ffill()\n",
    "office_data = office_temp.bfill()\n",
    "office_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:26.646542Z",
     "start_time": "2019-11-25T17:34:26.506875Z"
    }
   },
   "outputs": [],
   "source": [
    "# electricity\n",
    "df_elc = office_data[office_data['meter']==0]\n",
    "df_elc.drop(columns=['meter'], inplace=True)\n",
    "df_elc.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reading = df_elc['meter_reading']\n",
    "df_elc.loc['2016-07-07':'2016-07-14']['meter_reading'].plot(figsize=(15,8))\n",
    "# df_reading.plot(figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n",
    "    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:34.683917Z",
     "start_time": "2019-11-25T17:34:34.584682Z"
    }
   },
   "outputs": [],
   "source": [
    "# target_names = ['meter_reading', 'air_temperature']\n",
    "target_names = ['meter_reading', 'air_temperature'] # list(df_elc.columns)\n",
    "shift_days = 1\n",
    "shift_steps = shift_days * 24 # number of hours.\n",
    "df_targets = df_elc[target_names].shift(-shift_steps)\n",
    "df_targets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:46.551615Z",
     "start_time": "2019-11-25T17:34:46.366401Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = df_elc\n",
    "df_features.iloc[24:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:48.559185Z",
     "start_time": "2019-11-25T17:34:48.468604Z"
    }
   },
   "outputs": [],
   "source": [
    "# x_data\n",
    "x_data = df_features.values[0:-shift_steps]\n",
    "print(type(x_data))\n",
    "print('Feature Shape:', x_data.shape)\n",
    " \n",
    "# y_data\n",
    "y_data = df_targets.values[:-shift_steps]\n",
    "print(type(y_data))\n",
    "print('Target Shape:', y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:51.407187Z",
     "start_time": "2019-11-25T17:34:51.319310Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the input-signals for the training- and test-sets:\n",
    "num_data = len(x_data)\n",
    "train_split = 0.9\n",
    "num_train = int(train_split * num_data)\n",
    "num_test = num_data - num_train\n",
    "print('Training observations:', num_train)\n",
    "print('Validation observations:', num_test)\n",
    "x_train = x_data[0:num_train]\n",
    "x_test  = x_data[num_train:]\n",
    "print('Total observations:', (len(x_train) + len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:53.615009Z",
     "start_time": "2019-11-25T17:34:53.510613Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:54.148990Z",
     "start_time": "2019-11-25T17:34:54.096120Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:57.278521Z",
     "start_time": "2019-11-25T17:34:57.199657Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the output-signals for the training- and test-sets:\n",
    "y_train = y_data[0:num_train]\n",
    "y_test = y_data[num_train:]\n",
    "print('Total test observations:', (len(y_train) + len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:58.079226Z",
     "start_time": "2019-11-25T17:34:58.019992Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:34:58.873546Z",
     "start_time": "2019-11-25T17:34:58.792870Z"
    }
   },
   "outputs": [],
   "source": [
    "num_x_signals = x_data.shape[1]\n",
    "num_y_signals = y_data.shape[1]\n",
    "print('Number of input-signals:', num_x_signals, 'and shape:', x_train.shape )\n",
    "print('Number of output-signals:', num_y_signals, 'and shape', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:02.301988Z",
     "start_time": "2019-11-25T17:35:02.215645Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data[24:27] # I shifted the data by one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:03.236610Z",
     "start_time": "2019-11-25T17:35:03.115609Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Min and Max x_train data')\n",
    "print('    Min:', np.min(x_train))\n",
    "print('    Max:', np.max(x_train))\n",
    "x_scaler = MinMaxScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "print('Min and Max x_train_scaled data')\n",
    "print('    Min:', np.min(x_train_scaled))\n",
    "print('    Max:', np.max(x_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:06.311279Z",
     "start_time": "2019-11-25T17:35:06.240941Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test[24:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:06.839986Z",
     "start_time": "2019-11-25T17:35:06.771987Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:07.593967Z",
     "start_time": "2019-11-25T17:35:07.487849Z"
    }
   },
   "outputs": [],
   "source": [
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "print('Min and Max x_test data')\n",
    "print('    Min:', np.min(x_test))\n",
    "print('    Max:', np.max(x_test))\n",
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "print('Min and Max y_test_scaled data')\n",
    "print('    Min:', np.min(y_test_scaled))\n",
    "print('    Max:', np.max(y_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:10.398657Z",
     "start_time": "2019-11-25T17:35:10.334856Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x_train_scaled.shape)\n",
    "print(y_train_scaled.shape)\n",
    "print(x_test_scaled.shape)\n",
    "print(y_test_scaled.shape)\n",
    "y_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:13.379459Z",
     "start_time": "2019-11-25T17:35:13.326315Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x_train_scaled.shape)\n",
    "print(y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:14.262377Z",
     "start_time": "2019-11-25T17:35:14.195409Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n",
    "        \n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# reshape input to be [samples, time steps, features]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:16.258197Z",
     "start_time": "2019-11-25T17:35:16.175608Z"
    }
   },
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "batch_size = 72\n",
    "sequence_length = 24\n",
    "\n",
    "# create the batch-generator\n",
    "generator = batch_generator(batch_size, sequence_length)\n",
    "\n",
    "# test the batch-generator to see if it works \n",
    "x_batch, y_batch = next(generator)\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get a batch output where we have a batch size of xxx sequences, each sequence has yyy, and zzz input signals and zzz' output signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:20.194813Z",
     "start_time": "2019-11-25T17:35:19.847882Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 0   # First sequence in the batch.\n",
    "signal = 0  # First signal from the 20 input-signals.\n",
    "seq = x_batch[batch, :, signal]\n",
    "plt.plot(seq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:21.322199Z",
     "start_time": "2019-11-25T17:35:21.075582Z"
    }
   },
   "outputs": [],
   "source": [
    "seq = y_batch[batch, :, signal]\n",
    "plt.plot(seq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:21.579142Z",
     "start_time": "2019-11-25T17:35:21.468571Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T18:00:33.922049Z",
     "start_time": "2019-11-25T18:00:33.573178Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tf.keras.models import Sequential  # This does not work!\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:24.988553Z",
     "start_time": "2019-11-25T17:35:24.932194Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:35:26.101274Z",
     "start_time": "2019-11-25T17:35:25.703899Z"
    }
   },
   "outputs": [],
   "source": [
    "# model Architecture\n",
    "model = Sequential()\n",
    "model.add(GRU(units=512,\n",
    "              return_sequences=True,\n",
    "              input_shape=(None, num_x_signals,)))\n",
    "\n",
    "model.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -- start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T13:55:44.457463Z",
     "start_time": "2019-11-23T13:55:44.210488Z"
    }
   },
   "outputs": [],
   "source": [
    "# model Architecture\n",
    "model_s = Sequential()\n",
    "model_s.add(LSTM(units=4,\n",
    "              return_sequences=True,\n",
    "#               input_shape=(None, num_x_signals,)\n",
    "                 input_shape=(None, num_x_signals,)\n",
    "                ))\n",
    "\n",
    "model_s.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "\n",
    "model_s.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the other code\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=100,\n",
    "               return_sequences=True,\n",
    "               input_shape=(x_batch.shape[1], x_batch.shape[2])))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(num_y_signals))\n",
    "model_lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(x_batch, y_batch, epochs=2, \n",
    "                    batch_size=70, validation_data=(X_test, Y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "# # Training Phase\n",
    "\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=2,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# history = model.fit(X_train, Y_train, epochs=20, batch_size=70, validation_data=(X_test, Y_test), \n",
    "#                     callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "# # Training Phase\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:30.082478Z",
     "start_time": "2019-11-25T17:36:29.892754Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "    # Maybe use lower init-ranges.\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "\n",
    "    model.add(Dense(num_y_signals,\n",
    "                    activation='linear',\n",
    "                    kernel_initializer=init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:34.689750Z",
     "start_time": "2019-11-25T17:36:34.610604Z"
    }
   },
   "outputs": [],
   "source": [
    "warmup_steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:36.121589Z",
     "start_time": "2019-11-25T17:36:36.050354Z"
    }
   },
   "outputs": [],
   "source": [
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between y_true and y_pred,\n",
    "    but ignore the beginning \"warmup\" part of the sequences.\n",
    "    \n",
    "    y_true is the desired output.\n",
    "    y_pred is the model's output.\n",
    "    \"\"\"\n",
    "\n",
    "    # The shape of both input tensors are:\n",
    "    # [batch_size, sequence_length, num_y_signals].\n",
    "\n",
    "    # Ignore the \"warmup\" parts of the sequences\n",
    "    # by taking slices of the tensors.\n",
    "    y_true_slice = y_true[:, warmup_steps:, :]\n",
    "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
    "\n",
    "    # These sliced tensors both have this shape:\n",
    "    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n",
    "\n",
    "    # Calculate the MSE loss for each value in these tensors.\n",
    "    # This outputs a 3-rank tensor of the same shape.\n",
    "    loss = tf.losses.mean_squared_error(y_true_slice,\n",
    "                                        y_pred_slice)\n",
    "\n",
    "    # Keras may reduce this across the first axis (the batch)\n",
    "    # but the semantics are unclear, so to be sure we use\n",
    "    # the loss across the entire tensor, we reduce it to a\n",
    "    # single scalar with the mean function.\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:40.786040Z",
     "start_time": "2019-11-25T17:36:40.574855Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimization  \n",
    "optimizer = RMSprop(lr=1e-3)\n",
    "model.compile(loss=loss_mse_warmup, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:41.437241Z",
     "start_time": "2019-11-25T17:36:41.378444Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output is a tensor with an arbitrary batch size and arbitrary sequence length and two output signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:53.972230Z",
     "start_time": "2019-11-25T17:36:53.905461Z"
    }
   },
   "outputs": [],
   "source": [
    "path_checkpoint = '23_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                     monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:55.018482Z",
     "start_time": "2019-11-25T17:36:54.954950Z"
    }
   },
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:55.715092Z",
     "start_time": "2019-11-25T17:36:55.654462Z"
    }
   },
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:36:57.909814Z",
     "start_time": "2019-11-25T17:36:57.845693Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train  the Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T17:38:11.738586Z",
     "start_time": "2019-11-25T17:37:03.538837Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit_generator(generator=generator,\n",
    "                    epochs=2,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n",
    "                        y=np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(start_idx, model=model,length=100, train=True):\n",
    "    \"\"\"\n",
    "    Plot the predicted and true output-signals.\n",
    "    \n",
    "    :param start_idx: Start-index for the time-series.\n",
    "    :param length: Sequence-length to process and plot.\n",
    "    :param train: Boolean whether to use training- or test-set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        # Use training-data.\n",
    "        x = x_train_scaled\n",
    "        y_true = y_train\n",
    "    else:\n",
    "        # Use test-data.\n",
    "        x = x_test_scaled\n",
    "        y_true = y_test\n",
    "    \n",
    "    # End-index for the sequences.\n",
    "    end_idx = start_idx + length\n",
    "    \n",
    "    # Select the sequences from the given start-index and\n",
    "    # of the given length.\n",
    "    x = x[start_idx:end_idx]\n",
    "    y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "    # Input-signals for the model.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Use the model to predict the output-signals.\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    # The output of the model is between 0 and 1.\n",
    "    # Do an inverse map to get it back to the scale\n",
    "    # of the original data-set.\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
    "    \n",
    "    # For each output-signal.\n",
    "    for signal in range(len(target_names)):\n",
    "        # Get the output-signal predicted by the model.\n",
    "        signal_pred = y_pred_rescaled[:, signal]\n",
    "        \n",
    "        # Get the true output-signal from the data-set.\n",
    "        signal_true = y_true[:, signal]\n",
    "\n",
    "        # Make the plotting-canvas bigger.\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # Plot and compare the two signals.\n",
    "        plt.plot(signal_true, label='true')\n",
    "        plt.plot(signal_pred, label='pred')\n",
    "        \n",
    "        # Plot grey box for warmup-period.\n",
    "        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "        \n",
    "        # Plot labels etc.\n",
    "        plt.ylabel(target_names[signal])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "#     return y_pred_rescaled, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=4000, model=model, length=1000, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design network\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(units=100, input_shape=(None, num_x_signals,), return_sequences=True))\n",
    "model1.add(Dense(num_y_signals, activation='sigmoid'))\n",
    "model1.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model1.fit_generator(generator=generator,\n",
    "                    epochs=20,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_data,\n",
    "                    callbacks=callbacks)\n",
    "# plot history\n",
    "# pyplot.plot(history.history['loss'], label='train')\n",
    "# pyplot.plot(history.history['val_loss'], label='test')\n",
    "# pyplot.legend()\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=4000, model=model1, length=1000, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    " \n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train, y=y_train, \n",
    "         epochs=5, validation_data=(x_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
